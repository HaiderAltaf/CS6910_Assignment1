{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa2lBjao93XYJtVRhARXbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiderAltaf/CS6910_Assignment1/blob/branch-1/cs6910_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Functions needed for construction of our neural network\n",
        "\n"
      ],
      "metadata": {
        "id": "FIEvcvhjalfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights(weight_init, n, L, X_train, no_of_classes):\n",
        "  \n",
        "  Weights = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if weight_init ==\"random\":\n",
        "    temp = np.random.rand(n, len(X_train[0]))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.random.rand(n, n)\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.rand(no_of_classes, n)\n",
        "    Weights.append(temp)\n",
        "\n",
        "  if weight_init ==\"xavier\":\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(n, len(X_train[0]))) \n",
        "    Weights.append(temp)\n",
        "    for i in range(1, L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n,n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes, n)) \n",
        "    Weights.append(temp)\n",
        "\n",
        "  if weight_init ==3:\n",
        "    temp = np.zeros((n, len(X_train[0])))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.zeros((n, n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.zeros((no_of_classes, n))\n",
        "    Weights.append(temp)\n",
        "\n",
        "  \n",
        "  return Weights\n",
        "\n"
      ],
      "metadata": {
        "id": "o4pDXaK65aNO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biases(weight_init, n, L, y_train, no_of_classes):\n",
        "  \n",
        "  bias = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if weight_init ==\"random\":\n",
        "    for i in range(L-1):\n",
        "      temp = np.random.rand(n)  # for schochastic GD\n",
        "      #temp = np.random.rand(len(y_train),n)  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "    \n",
        "    temp = np.random.rand(no_of_classes)   # for schochastic GD\n",
        "    #temp = np.random.rand(len(y_train), no_of_classes)   # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if weight_init ==\"xavier\":\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "    for i in range(L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n))   # for schochastic GD\n",
        "      #temp  = np.random.uniform(-limit, limit, size=(len(y_train),n))  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes))   # for schochastic GD\n",
        "    #temp = np.random.uniform(-limit, limit, size=(len(y_train),no_of_classes)) # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if weight_init ==3:\n",
        "    for i in range(L-1):\n",
        "      temp = np.zeros(n)\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.zeros(no_of_classes)\n",
        "    bias.append(temp)\n",
        "\n",
        "  \n",
        "  return bias\n"
      ],
      "metadata": {
        "id": "Mp2HVC8eUEvK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Ki8Z7zdZE8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNE8AXdi7fB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "SNe2k9o5fJCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(a):\n",
        "  a = np.float128(a)\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "def tanh(a):\n",
        "  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "\n",
        "def ReLu(a, weight_init):\n",
        "  i=0\n",
        "  for ele in a:\n",
        "    a[i] = max(0,ele)\n",
        "    i+=1\n",
        "  if weight_init == \"random\":\n",
        "    return a - max(a)\n",
        "  \n",
        "  return a\n",
        "\n",
        "def softmax(a):\n",
        "  # temp = np.zeros_like(a)\n",
        "  # for i in range(len(temp)):\n",
        "  #   temp[i] = np.float128(a[i])\n",
        "  #exp_logits = np.exp(a)\n",
        "  return np.exp(a)/np.sum(np.exp(a))   \n",
        "\n",
        "def der_softmax(a):\n",
        "  return softmax(a)*(1- softmax(a))\n",
        "  \n",
        "def der_sigmoid(a):\n",
        "  return sigmoid(a)*(1-sigmoid(a))\n",
        "\n",
        "def der_tanh(a):\n",
        "  return 1-(tanh(a)*tanh(a))\n",
        "\n",
        "def der_ReLu(a):\n",
        "\n",
        "  # it will create a matrix of same dimension as of a.\n",
        "  gradient = np.zeros_like(a)  \n",
        "  # sets the entries of gradient to 1 where the corresponding entries of x>=0\n",
        "  gradient[a >= 0] = 1\n",
        "  return gradient"
      ],
      "metadata": {
        "id": "zQeYcLR3eZXr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions"
      ],
      "metadata": {
        "id": "lVMnOZ5LI-Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_dash, y_train, X_train):\n",
        "  losses = -np.log(y_dash[y_train])/len(X_train)\n",
        "  return losses\n",
        "\n",
        "def MSE_loss(y_dash, y_train, X_train):\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "  losses = (np.sum((y_dash - y_train_modified)**2))/len(X_train)\n",
        "  return losses"
      ],
      "metadata": {
        "id": "7D-NSGVpI9jt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset. Show each sample class in wandb"
      ],
      "metadata": {
        "id": "SfJidXdSY0F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "RntaGW8UZaXC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3AuAaMUZbUw",
        "outputId": "caab1e01-5627-4cb5-9c9b-59ff23c4e692"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=c1b503c4f2738a64239474ae411d6950dcabd4be4a2beedd8bdc3409a1f603be\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255\n",
        "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVWesfE9ZgLw",
        "outputId": "79f9975b-014e-491a-cee0-a1f8c10a1441"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = int(len(X_train)*0.1)\n",
        "\n",
        "# randomly shuffle the indices of the data\n",
        "shuffled_indices = np.random.permutation(len(X_train))\n",
        "\n",
        "# split the shuffled data into training and validation sets\n",
        "train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]\n",
        "X_train, X_validation = X_train[train_indices], X_train[validation_indices]\n",
        "y_train, y_validation = y_train[train_indices], y_train[validation_indices]"
      ],
      "metadata": {
        "id": "utJgTTC3ZuY0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hid_layer = int(input(\"Enter the number of Hidden + outer layer: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBS5D-MMaIOD",
        "outputId": "950c4414-9c93-43b3-ea7d-d77e8f7a5c68"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of Hidden + outer layer: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_neuron = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PamXMq9AaKVq",
        "outputId": "ffa2b631-bfdb-4c0a-e16e-816db545f459"
      },
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the numbers of neuron in each hidden layer: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_classes = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "y0iWHQYlaQaD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_init = input(\"For random weights initialisation enter random and for xavier enter xavier: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rhbT_MXaVO3",
        "outputId": "07df7c12-ee57-495d-9987-31802692640c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For random weights initialisation enter random and for xavier enter xavier: random\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(entity= \"am22s020\", project=\"cs6910_trial\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "x0HaV8ppZGKD",
        "outputId": "29b9efe6-da56-45f0-f78b-7e66bb364adb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mam22s020\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230307_044456-vypf7285</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/am22s020/cs6910_trial/runs/vypf7285' target=\"_blank\">deep-thunder-28</a></strong> to <a href='https://wandb.ai/am22s020/cs6910_trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/am22s020/cs6910_trial' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/am22s020/cs6910_trial/runs/vypf7285' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial/runs/vypf7285</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/am22s020/cs6910_trial/runs/vypf7285?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7ee3003100>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_sample():\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
        "                'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "            \n",
        "  no_of_classes = len(class_names)\n",
        "\n",
        "  list_of_images  = []   # to give to the wandb\n",
        "\n",
        "  for i in range(no_of_classes):\n",
        "    \n",
        "      # Find the index of the first image of each class\n",
        "      idx = np.where(y_train == i)[0][0]\n",
        "      \n",
        "      # Plot the image\n",
        "      image = X_train[idx].reshape(28,28)\n",
        "      list_of_images.append((image, class_names[i]))\n",
        "\n",
        "  # Plot the images in a grid\n",
        "  fig, axes = plt.subplots(1, no_of_classes, figsize=(12,5))\n",
        "  for i in range(no_of_classes):\n",
        "      image, label = list_of_images[i]\n",
        "      axes[i].imshow(image, cmap='gray')\n",
        "      axes[i].set_title(label)\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  wandb.log({\"Question 1\": [wandb.Image(img, caption=caption) for img, caption in list_of_images]})\n"
      ],
      "metadata": {
        "id": "cylXBOBnY9Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 \n",
        "\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
      ],
      "metadata": {
        "id": "MRsgs5EJypw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(Weights, bias, x_input, hid_layer, acti_fun):\n",
        "  \n",
        "  L = hid_layer\n",
        "  h = x_input\n",
        "  a_out = []\n",
        "  h_out = []\n",
        "  h_out.append(h)\n",
        "  \n",
        "  ## for hidden layers\n",
        "  for k in range(L-1):\n",
        "    a = np.matmul(Weights[k], h) + bias[k]\n",
        "    a_out.append(a)\n",
        "    ## default activation function is sigmoid \n",
        "    if acti_fun == 'sigmoid':\n",
        "      h = sigmoid(a)\n",
        "    elif acti_fun == 'ReLu':\n",
        "      h = ReLu(a)\n",
        "    elif acti_fun == 'tanh':\n",
        "      h = tanh(a)\n",
        "    h_out.append(h)\n",
        "\n",
        "  ## In outer layer softmax function\n",
        "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
        "  a_out.append(a)\n",
        "  y_dash = softmax(a)\n",
        "  \n",
        "\n",
        "  return a_out, h_out, y_dash\n"
      ],
      "metadata": {
        "id": "jOypoQjEt81D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-3\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "    sgd\n",
        "    momentum based gradient descent\n",
        "    nesterov accelerated gradient descent\n",
        "    rmsprop\n",
        "    adam\n",
        "    nadam \n"
      ],
      "metadata": {
        "id": "ZNSQ35cHX1WO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward propagation"
      ],
      "metadata": {
        "id": "z2DZ8Ewi05Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Weights = weights('random', 32, 3, X_train, 10)"
      ],
      "metadata": {
        "id": "TqNajG06a2AF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias = biases('random', 32, 3, y_train, 10)"
      ],
      "metadata": {
        "id": "zPQxJsVza24l"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                         hid_layer, loss_fu, acti_fun, L2_decay):\n",
        "  \n",
        "  L = hid_layer\n",
        "  grad_W = [0]*L\n",
        "  grad_b = [0]*L\n",
        "\n",
        "  \n",
        "  ## change each y_train into an array of 10 values\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "    \n",
        "  if loss_fu =='cross_entropy':\n",
        "    output_gradient = -(y_train_modified - y_dash) #+ L2_decay*np.sum(Weights[0]) + L2_decay*np.sum(Weights[1])+ L2_decay*np.sum(Weights[2])\n",
        "  else:\n",
        "    output_gradient = (y_dash-y_train_modified )*der_softmax(a_out[L-1]) # + L2_decay*np.sum(Weights[0]) + L2_decay*np.sum(Weights[1])+ L2_decay*np.sum(Weights[2])\n",
        "\n",
        "  for k in range(L, 0, -1):\n",
        "\n",
        "    ## compute gradients w.r.t parameters\n",
        "    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), \n",
        "                           h_out[k-1].reshape(1,len(h_out[k-1]))) \n",
        "    grad_W[k-1] = W_gradient\n",
        "\n",
        "    b_gradients = output_gradient \n",
        "    grad_b[k-1] = b_gradients\n",
        "   \n",
        "    if k==1:\n",
        "      continue\n",
        "    ## compute gradients w.r.t layer below\n",
        "    weight = Weights[k-1]\n",
        "    h_gradient = np.matmul(weight.T, output_gradient)\n",
        "\n",
        "    ## compute the gradient of pre activation layer\n",
        "    if acti_fun == 'sigmoid':\n",
        "      output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))\n",
        "    elif acti_fun == 'ReLu':\n",
        "      output_gradient = np.multiply(h_gradient, der_ReLu(a_out[k-2]))\n",
        "    elif acti_fun == 'tanh':\n",
        "     output_gradient = np.multiply(h_gradient, der_tanh(a_out[k-2]))\n",
        "    \n",
        "\n",
        "  return grad_W, grad_b\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6s4dnd6ykc4"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(Weights[0])+ np.zeros(5)+np.sum(Weights[1])+np.sum(Weights[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkBGwi33c_5N",
        "outputId": "9dfb0895-a9ec-4405-b480-db4ea66e652a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13139.59580177, 13139.59580177, 13139.59580177, 13139.59580177,\n",
              "       13139.59580177])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Loss"
      ],
      "metadata": {
        "id": "HIoiBYBjdF-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_loss(X, Y, Weights, bias, hid_layer, loss_fu,\n",
        "               L2_decay, X_train, X_validation, acti_fun):\n",
        "  \n",
        "  L = hid_layer\n",
        "  loss = 0\n",
        " # i = 0\n",
        "  for x,y in zip(X, Y):\n",
        "      _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "      #i+=1\n",
        "      \n",
        "      if loss_fu == 'cross_entropy':\n",
        "        loss_iter = cross_entropy_loss(y_dash, y, X)\n",
        "      elif loss_fu == 'mse':\n",
        "        loss_iter = MSE_loss(y_dash, y_train, X)\n",
        "          \n",
        "      loss += loss_iter\n",
        "\n",
        "      # if i==len(X_validation):\n",
        "      #   break\n",
        "\n",
        "\n",
        "  # Adding L2 regularization loss after an epochS\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 2*L2_decay*np.sum(Weights[i]**2)/len(X)\n",
        "\n",
        "  return loss\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "Y9-hsiQddEvS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss(X_test, y_test, Weights, bias, hid_layer, 'cross_entropy', 0.5, X_train, X_validation, acti_fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1U7NZPtfR6X",
        "outputId": "076291ad-1ae6-42c6-91ff-085e6d1493d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2371012814240635754"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy"
      ],
      "metadata": {
        "id": "7Jf_j_EtFJhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_accuracy(X, Y, Weights, bias, hid_layer, acti_fun):\n",
        "  \n",
        "  L = hid_layer\n",
        "  y_pred = np.zeros((len(X), 10))\n",
        "  i=0\n",
        "  for x in X:\n",
        "    _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    y_pred[i] = y_dash\n",
        "    i+=1\n",
        "\n",
        "  correct = 0\n",
        "  for array,y in zip(y_pred, Y):\n",
        "    if np.argmax(array)==y:\n",
        "      correct+=1\n",
        "  accuracy = correct*100/len(X)\n",
        "\n",
        "  return  accuracy"
      ],
      "metadata": {
        "id": "4vQxChQuUQDP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, acti_fun)"
      ],
      "metadata": {
        "id": "picBbO9RYcK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abb715b-44d4-489b-f221-9b50de06ac18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.3"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Gradient Descent \n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent.\n",
        "if batch_size = Number of samples, the algorithm will be vanilla gradient descent\n"
      ],
      "metadata": {
        "id": "KsS21O64Kvzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
        "                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L = hid_layer\n",
        "  n = no_of_neuron\n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x,y in zip(X_train,y_train):\n",
        "\n",
        "    ##x,y = np.float128(x), np.float128(y)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "\n",
        "    ## updating loss after each sample(not using this)\n",
        "    \n",
        "    # if loss_fu == 'cross_entropy':\n",
        "    #   loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy loss \n",
        "    # else:\n",
        "    #   loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss\n",
        "\n",
        "    # loss += loss_iter\n",
        "\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "\n",
        "    ## Adding the gradients of weights and biases\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "    \n",
        "    num_points_seen+=1\n",
        "    \n",
        "    if num_points_seen%batch_size == 0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
        "      # Weights updates\n",
        "      \n",
        "      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      \n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "  # Adding L2 regularization loss after an epochS\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "    \n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "  return Weights, bias, loss\n"
      ],
      "metadata": {
        "id": "lF615oFSKuv-"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  Weights, bias, loss = gradient_descent(0.01, Weights, bias, hid_layer, no_of_neuron,\n",
        "                                         y_train, X_train, 25, 0.5,'cross_entropy', 'sigmoid')\n",
        "  print(i, loss)\n",
        "  val_accuracy  = Model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'sigmoid')\n",
        "  test_accuracy = Model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'sigmoid')\n",
        "  val_loss      = model_loss(X_train, y_train, Weights, bias, hid_layer, 'cross_entropy', 0, X_train, X_validation, 'sigmoid')\n",
        "  test_loss     = model_loss(X_test, y_test, Weights, bias, hid_layer, 'cross_entropy', 0, X_train, X_validation, 'sigmoid')\n",
        "  print('val_accuracy',val_accuracy,'test_accuracy',test_accuracy,'val_loss', val_loss,'test_loss',test_loss)\n",
        "# Finish the WandB run\n",
        "#wandb.finish()"
      ],
      "metadata": {
        "id": "O11C1AEYPgFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, 'sigmoid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F60POmeeW-51",
        "outputId": "96a673d2-a031-4e11-94ef-c3d8304c4a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84.66666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Momentum based Gradient Descent\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "IK6JEr8ORuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev_uw = weights(3, n, L, X_train, no_of_classes)\n",
        "prev_ub = biases(3, n, L, y_train, no_of_classes)\n",
        " "
      ],
      "metadata": {
        "id": "A_u9VRqbBuly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Momentum_GD(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
        "                no_of_neuron, X_train, y_train,learning_rate,\n",
        "                batch_size, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  beta = 0.9\n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L = hid_layer\n",
        "  n = no_of_neuron\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x,y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # if loss_fu == 'cross_entropy':\n",
        "    #   loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy loss\n",
        "    # else:\n",
        "    #   loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    \n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "\n",
        "    ## Adding the gradients of weights and biases\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      ## momentum based wight updates\n",
        "      uw = [prev_uw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "      ub = [prev_ub[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "      \n",
        "      ## Weights and biases updates\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - uw[i] for i in range(len(uw))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - ub[i] for i in range(len(ub))]\n",
        "\n",
        "      # assign present to the history \n",
        "      prev_uw = uw\n",
        "      prev_ub = ub\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "  \n",
        "    # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "  return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "N9hSwUCaRVVz"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Momentum_GD(Weights, bias, hid_layer, X_train, y_train, 0.9, 0.0001, 25, 0.0005, loss_fu, acti_fun)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-rcveBQD_w",
        "outputId": "4797adf0-a83f-4d32-a5ea-4b37913b6caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.5466685496257547951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, hid_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afKsmhTauV2e",
        "outputId": "854e4044-b290-4ef6-9658-ea42723ff4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53.15"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nesterov Accelerated Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "KcBoS2WQeF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev_vw = weights(3, 32, 3, X_train, 10)\n",
        "prev_vb = biases(3, 32, 3, y_train, 10)"
      ],
      "metadata": {
        "id": "i1JfKzJWBAb7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
        "        no_of_neuron, X_train, y_train,learning_rate,\n",
        "        batch_size, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  beta = 0.9\n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L = hid_layer\n",
        "  n = no_of_neuron\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  # do partial updates\n",
        "  v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]\n",
        "  v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]\n",
        "    bias    = [bias[i]-v_b[i] for i in range(len(bias))]\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights, \n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "\n",
        "    ## Look Ahead\n",
        "    ## Adding the gradients of weights and biases\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      ## momentum based wight updates\n",
        "      vw = [prev_vw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "      vb = [prev_vb[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "\n",
        "      ## Weights and biases updates\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - vw[i] for i in range(len(vw))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - vb[i] for i in range(len(vb))]\n",
        "\n",
        "      # assign present to the history \n",
        "      prev_uw = vw\n",
        "      prev_ub = vb\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "  # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "  return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "EhjA6UuceE_v"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias, loss = NAG(Weights, bias, 3, 32, X_train, y_train,\n",
        "                    0.0001, 25, 0, 'cross_entropy', 'sigmoid')\n",
        "  "
      ],
      "metadata": {
        "id": "yFL7QpxmVKoZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Gradient(AdaGrad) based Gradient Descent- Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "ti8ADwESXAGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# v_b = biases(3, n, L, y_train, no_of_classes)\n"
      ],
      "metadata": {
        "id": "-PVkK0IgAoeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaGrad(Weights, bias, hid_layer, no_of_neuron, X_train, y_train, eps,\n",
        "            learning_rate, batch_size, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L = hid_layer\n",
        "  n = no_of_neuron\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      #compute intermediate values\n",
        "      v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]\n",
        "      v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]\n",
        "\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "  # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "\n",
        "  return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "ktWLOxF2WnnD"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = AdaGrad(Weights, bias, hid_layer, no_of_neuron, X_train, y_train, 1e-8,\n",
        "                        0.0001, 25, L2_decay, loss_fu, acti_fun)\n",
        "  "
      ],
      "metadata": {
        "id": "zmF6kRdHX6-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Root Mean Squared Propagation(RMSProp) Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "Qwy4m-glvWKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# v_b = biases(3, n, L, y_train, no_of_classes)\n"
      ],
      "metadata": {
        "id": "qQd_wf2TAWpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSProp(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
        "            X_train, y_train, learning_rate, batch_size,\n",
        "            L2_decay,loss_fu, acti_fun):\n",
        "  \n",
        "  eps = 1e-10\n",
        "  beta = 0.9\n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L = hid_layer\n",
        "  n = no_of_neuron\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "    \n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      #compute intermediate values\n",
        "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "      ## Weights and biases updates\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "  # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "\n",
        "  return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "-HH3ZkpwvcJS"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MiniBatch RMSProp\n",
        "%%time\n",
        "Weights, bias = RMSProp(Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
        "                        learning_rate, batch_size, L2_decay,loss_fu, acti_fun)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NjTeTrU1Nbv",
        "outputId": "c370d7a1-a031-461b-c325-d86350760575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.3039213502909686143\n",
            "CPU times: user 40.5 s, sys: 79.4 ms, total: 40.6 s\n",
            "Wall time: 49.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Delta(AdaDelta) gradient descent - Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "6CLGHkDl4t5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# u_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# u_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "# v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# v_b = biases(3, n, L, y_train, no_of_classes)\n"
      ],
      "metadata": {
        "id": "tERzRzZr_q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
        "             no_of_neuron, X_train, y_train, eps, batch_size,\n",
        "             beta, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L, n = hid_layer, no_of_neuron\n",
        "  \n",
        "  \n",
        "\n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "    \n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "        ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      #compute intermediate values\n",
        "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "      del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]\n",
        "      del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]\n",
        "\n",
        "      u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]\n",
        "      u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]\n",
        "\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - del_b[i] for i in range(len(del_b))]\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "  # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "\n",
        "  return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "FAnqIPVXZj9q"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## MiniBatch AdaDelta\n",
        "Weights, bias = AdaDelta(Weights, bias, hid_layer, no_of_neuron, X_train, y_train, 1E-8, 25, 0.9, 0.0005, loss_fu, acti_fun)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTGhRTO5aBdv",
        "outputId": "9a40e217-1173-4ac0-a358-b910a76c4d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.3038936342067151898\n",
            "CPU times: user 38.6 s, sys: 64 ms, total: 38.7 s\n",
            "Wall time: 41.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive moments(Adam) Gradient Descent- MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "BklUlyYNE2V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eps = 1e-10\n",
        "#   beta1 = 0.9\n",
        "#   beta2 = 0.999\n",
        "  \n",
        "#   m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "#   m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "#   v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "#   v_b = biases(3, n, L, y_train, no_of_classes)"
      ],
      "metadata": {
        "id": "PDIGDxZ4wCKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam(eps, epoch, beta1, beta2, m_w, m_b, v_w, v_b, Weights,\n",
        "         bias, hid_layer, no_of_neuron, X_train, y_train, \n",
        "         batch_size, learning_rate, L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L, n = hid_layer, no_of_neuron\n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "    \n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      #compute intermediate values\n",
        "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - learning_rate*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - learning_rate*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "    # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "  return Weights, bias, loss\n"
      ],
      "metadata": {
        "id": "pCvLuniEE1Z_"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Adam(Weights, bias, hid_layer, no_of_neuron, X_train, y_train, batch_size, learning_rate, L2_decay, loss_fu, acti_fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kaWlRUbkeTw",
        "outputId": "fb861428-8097-4789-e7bd-8614ad860b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.4280302102562264343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RD_lhshLj0Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NAG + Adam = NAdam Gradient descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "nGmz0pRpBPaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eps = 1e-10\n",
        "# beta1 = 0.9\n",
        "# beta2 = 0.999\n",
        "\n",
        "# m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "# v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "# v_b = biases(3, n, L, y_train, no_of_classes)\n"
      ],
      "metadata": {
        "id": "GHLwISFxvPfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAdam(eps, epoch, beta1, beta2, m_w, m_b, v_w, v_b, Weights, bias,\n",
        "          hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
        "          learning_rate,L2_decay, loss_fu, acti_fun):\n",
        "  \n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  L, n = hid_layer, no_of_neuron\n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x, y in zip(X_train, y_train):\n",
        "\n",
        "    #x = np.float128(x)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun)\n",
        "    \n",
        "    # #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    # loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    # loss += loss_iter\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights,\n",
        "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
        "    \n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "    num_points_seen +=1\n",
        "\n",
        "    if num_points_seen%batch_size==0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "      #compute intermediate values\n",
        "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "      # Weights updates\n",
        "      Weights = [Weights[i] - (learning_rate/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - (learning_rate/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "   # Training loss of an epoch\n",
        "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
        "                     L2_decay, X_train, X_validation, acti_fun)\n",
        "\n",
        "    # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 0.5*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "  #print(epoch, loss)\n",
        "\n",
        "  return Weights, bias, loss\n"
      ],
      "metadata": {
        "id": "GLRgSWWOGSeZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = NAdam(Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
        "                      batch_size, learning_rate, L2_decay, loss_fu, acti_fun)"
      ],
      "metadata": {
        "id": "KYpytPVUn4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-4"
      ],
      "metadata": {
        "id": "uh5tIV_XaOyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep_config = {\"name\": \"complete-sweep\", \"method\": \"grid\"}   # may use random search if computation power is less\n",
        "# sweep_config[\"metric\"] = {\"name\": \"loss\", \"goal\": \"minimize\"}\n",
        "\n",
        "# parameters_dict = {\n",
        "#               \"max_epochs\": {\"values\": [5, 10]},\n",
        "#               \"number_of_hidden_layer\": {\"values\": [3, 4, 5]},  \n",
        "#               \"size_hidden_layer\": {\"values\": [32, 64, 128]},           \n",
        "#               \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "#               \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\",\"rmsprop\",\"adam\",\"nadam\"]},\n",
        "#               \"batch_size\": {\"values\": [16, 32, 64]}, \n",
        "#               \"weight_init\": {\"values\": [\"random\", \"xavier\"]} ,\n",
        "#               \"activation\": {\"values\": [\"Sigmoid\", \"tanh\", \"ReLu\"]}, \n",
        "#               \"loss\": {\"values\": [\"CrossEntropy\", \"SquaredError\"]}, \n",
        "#                 }\n",
        "# sweep_config[\"parameters\"] = parameters_dict\n"
      ],
      "metadata": {
        "id": "3q9QqPQGaNwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_NN(config=sweep_config):\n",
        "#       with wandb.init(config = config):\n",
        "#         config = wandb.init().config\n",
        "#         wandb.run.name = \"e_{}_nhl_{}_shl_{}_lr_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}\".format(config.max_epochs,\n",
        "#                                                                       config.number_of_hidden_layer,\n",
        "#                                                                       config.size_hidden_layer,\n",
        "#                                                                       config.learning_rate,\n",
        "#                                                                       config.optimizer,\n",
        "#                                                                       config.batch_size,\n",
        "#                                                                       config.weight_init,\n",
        "#                                                                       config.activation,\n",
        "#                                                                       config.loss)\n",
        "        \n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "R-YFMpOffOOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_NN():\n",
        "  \n",
        "  # default values\n",
        "  config_defaults = {\n",
        "        'max_epochs': 5,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 1e-3,\n",
        "        'acti_fun': 'sigmoid',\n",
        "        'optimizer': 'sgd',\n",
        "        'weight_init': 'random',\n",
        "        'L2_decay': 0,\n",
        "        'no_of_neuron': 16,\n",
        "        'hid_layer': 3,\n",
        "        'loss_fu':'cross_entropy'\n",
        "    }\n",
        "  \n",
        "  # initialize wandb\n",
        "  wandb.init(config=config_defaults)\n",
        "\n",
        "  # config is a data structure that holds hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "\n",
        "  # Local variables, values obtained from wandb config\n",
        "  no_of_neuron = config.no_of_neuron\n",
        "  hid_layer = config.hid_layer\n",
        "  weight_init = config.weight_init\n",
        "  max_epochs = config.max_epochs\n",
        "  batch_size = config.batch_size\n",
        "  learning_rate = config.learning_rate\n",
        "  acti_fun = config.acti_fun\n",
        "  L2_decay = config.L2_decay\n",
        "  optimizer = config.optimizer\n",
        "  loss_fu = config.loss_fu\n",
        "\n",
        "  wandb.run.name  = \"e_{}_nhl_{}_shl_{}_lr_{}_opt_{}_bs_{}_W_{}_af_{}_loss_{}\".format(max_epochs,\n",
        "                                                                              hid_layer,\n",
        "                                                                              no_of_neuron,\n",
        "                                                                              learning_rate,\n",
        "                                                                              optimizer,\n",
        "                                                                              batch_size,\n",
        "                                                                              weight_init,\n",
        "                                                                              acti_fun,\n",
        "                                                                              loss_fu)\n",
        "                                                                                  \n",
        "  \n",
        "  print(wandb.run.name )\n",
        "\n",
        "  no_of_classes = len(np.unique(y_train))\n",
        "  Weights = weights(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
        "  bias = biases(weight_init, no_of_neuron, hid_layer, y_train, no_of_classes)\n",
        "  eps = 1e-10\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  L, n = hid_layer, no_of_neuron\n",
        "  \n",
        "  prev_uw = weights(3, n, L, X_train, no_of_classes)\n",
        "  prev_ub = biases(3, n, L, y_train, no_of_classes)\n",
        " \n",
        "  prev_vw = weights(3, n, L, X_train, no_of_classes)\n",
        "  prev_vb = biases(3, n, L, y_train, no_of_classes)\n",
        "  \n",
        "  m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "\n",
        "    if optimizer == 'sgd':\n",
        "      Weights, bias, loss = gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
        "                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun)\n",
        "    elif optimizer == 'momentum':\n",
        "      Weights, bias, loss = Momentum_GD(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
        "                                        no_of_neuron, X_train, y_train,learning_rate,\n",
        "                                        batch_size, L2_decay, loss_fu, acti_fun)\n",
        "    elif optimizer == 'nesterov':\n",
        "      Weights, bias, loss = NAG(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
        "                                no_of_neuron, X_train, y_train,learning_rate,\n",
        "                                batch_size, L2_decay, loss_fu, acti_fun)\n",
        "    elif optimizer == 'rmsprop':\n",
        "      Weights, bias, loss = RMSProp(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
        "                                    X_train, y_train, learning_rate, batch_size,\n",
        "                                    L2_decay,loss_fu, acti_fun)\n",
        "    elif optimizer == 'adam':\n",
        "      Weights, bias, loss = Adam(eps, beta1, beta2, m_w, m_b, v_w, v_b, Weights,\n",
        "                                  bias, hid_layer, no_of_neuron, X_train, y_train, \n",
        "                                  batch_size, learning_rate, L2_decay, loss_fu, acti_fun)\n",
        "    elif optimizer == 'nadam':   \n",
        "      Weights, bias, loss = NAdam(eps, beta1, beta2, m_w, m_b, v_w, v_b, Weights, bias,\n",
        "                                  hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
        "                                  learning_rate,L2_decay, loss_fu, acti_fun)\n",
        "\n",
        "    print(epoch, loss)\n",
        "\n",
        "  \n",
        "    val_accuracy  = Model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, acti_fun)\n",
        "    train_accuracy = Model_accuracy(X_test, y_test, Weights, bias, hid_layer, acti_fun)\n",
        "    val_loss      = model_loss(X_validation, y_validation, Weights, bias, hid_layer, loss_fu, L2_decay, X_train, X_validation, acti_fun)\n",
        "    train_loss     = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu, L2_decay, X_train, X_validation, acti_fun)\n",
        "    \n",
        "  \n",
        "    wandb.log({\"validation accuracy\": val_accuracy, \"train accuracy\": train_accuracy, \"validation loss\": val_loss, \"train loss\": train_loss, 'epoch': epoch})\n",
        "    \n",
        "  wandb.run.name \n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "\n",
        "  return Weights, bias, loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OJ_1yvk0HuGk"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#W&B Sweep\n",
        "\n",
        "In this cell, we set up the configurations for the various hyperparameters and use the Sweeps feature to find the combination that gives us the highest validation accuracy.\n"
      ],
      "metadata": {
        "id": "3NoPTtMACUUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\"name\": \"cs6910_trial_2\", \"method\": \"grid\"}   # may use random search if computation power is less\n",
        "sweep_config[\"metric\"] = {\"name\": \"val_loss\", \"goal\": \"minimize\"}\n",
        "\n",
        "parameters_dict = {\n",
        "              \"max_epochs\": {\"values\": [5, 10]},\n",
        "              \"hid_layer\": {\"values\": [3, 4, 5]},  \n",
        "              \"no_of_neuron\": {\"values\": [32, 64, 128]},           \n",
        "              \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "              \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\",\"rmsprop\",\"adam\",\"nadam\"]},\n",
        "              \"batch_size\": {\"values\": [16, 32, 64]}, \n",
        "              \"weight_init\": {\"values\": [\"random\", \"xavier\"]} ,\n",
        "              \"L2_decay\": {\"values\": [0, 0.0005, 0.5]} ,\n",
        "              \"acti_fun\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLu\"]}, \n",
        "              \"loss_fu\": {\"values\": [\"cross_entropy\", \"mse\"]}, \n",
        "                }\n",
        "sweep_config[\"parameters\"] = parameters_dict\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"am22s020\", project=\"cs6910_trial_3\")\n",
        "wandb.agent(sweep_id, train_NN, count=1)"
      ],
      "metadata": {
        "id": "4XSwvYkDYuzZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "1aeeeaa8-33d7-4f6a-b5ce-63c6dc7029a7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 1us9ly1l\n",
            "Sweep URL: https://wandb.ai/am22s020/cs6910_trial_3/sweeps/1us9ly1l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fmxm41x7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tacti_fun: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fu: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_neuron: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nesterov\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230308_052759-fmxm41x7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/am22s020/cs6910_trial_3/runs/fmxm41x7' target=\"_blank\">revived-sweep-1</a></strong> to <a href='https://wandb.ai/am22s020/cs6910_trial_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/am22s020/cs6910_trial_3/sweeps/1us9ly1l' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial_3/sweeps/1us9ly1l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/am22s020/cs6910_trial_3' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/am22s020/cs6910_trial_3/sweeps/1us9ly1l' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial_3/sweeps/1us9ly1l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/am22s020/cs6910_trial_3/runs/fmxm41x7' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial_3/runs/fmxm41x7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nhl_3_shl_32_lr_0.001_opt_nesterov_bs_16_W_random_af_sigmoid_loss_cross_entropy\n",
            "0 2.302585092991647737\n",
            "1 2.3025850929928566972\n",
            "2 2.3025850929935658554\n",
            "3 2.3025850929938316166\n",
            "4 2.3025850929939351024\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>test loss</td><td>▁▁▁▁▁</td></tr><tr><td>validation accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>validation loss</td><td>▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>test accuracy</td><td>10.0</td></tr><tr><td>test loss</td><td>2.30259</td></tr><tr><td>validation accuracy</td><td>10.01667</td></tr><tr><td>validation loss</td><td>2.30259</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">revived-sweep-1</strong> at: <a href='https://wandb.ai/am22s020/cs6910_trial_3/runs/fmxm41x7' target=\"_blank\">https://wandb.ai/am22s020/cs6910_trial_3/runs/fmxm41x7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230308_052759-fmxm41x7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}