{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPspE6bBOPx7oz/pK97UnrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiderAltaf/Gradient-Descent-Algorithm-and-its-variants/blob/branch-1/Question_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
      ],
      "metadata": {
        "id": "bFTS3dpS7kRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwKm8spf7erm"
      },
      "outputs": [],
      "source": [
        "L = int(input(\"Enter the number of Hidden + outer layer: \"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
      ],
      "metadata": {
        "id": "lJavLjVk7pp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_classes = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "3Nf4Ih0x7sWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "choice = int(input(\"For random weights initialisation enter 1 and for xavier enter 2: \"))"
      ],
      "metadata": {
        "id": "WZdM32ff7vXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(Weights, bias, x_input, L):\n",
        "  \"\"\"\n",
        "  x_input = an image from the X_train\n",
        "  L= number of layers except input layer\n",
        "  Weights and bias are parameters\n",
        "\n",
        "  This function will take x_input and give\n",
        "   a_out = pre-activation value of each neuron\n",
        "   h_out = after applying activation function in each neuron, it will be input to next layer\n",
        "   y_dash = the output value of y label in terms of probability(as we are using softmax in outer layer)\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  h = x_input\n",
        "  a_out = [] \n",
        "  h_out = []\n",
        "  h_out.append(h)\n",
        "  \n",
        "  ## for hidden layers\n",
        "  for k in range(L-1):\n",
        "    #a = np.dot(h, Weights[k]) + bias[k]\n",
        "    a = np.matmul(Weights[k], h) + bias[k]\n",
        "    a_out.append(a)\n",
        "    ## default activation function is sigmoid \n",
        "    h = sigmoid(a)\n",
        "    h_out.append(h)\n",
        "\n",
        "  ## In outer layer softmax function\n",
        "  #a = np.dot(h, Weights[L-1]) + bias[L-1]\n",
        "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
        "  a_out.append(a)\n",
        "  y_dash = softmax(a)\n",
        "  \n",
        "\n",
        "  return a_out, h_out, y_dash\n"
      ],
      "metadata": {
        "id": "vBWjDM7F7yIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}