{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/0Q0ZankAc3aflnb/pECw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiderAltaf/Gradient-Descent-Algorithm-and-its-variants/blob/main/cs6910_assig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ],
      "metadata": {
        "id": "-5cMQpFel2pj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O_-ut5eWlniQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ4uiwM-V1O0",
        "outputId": "4781aa76-558a-4ae1-84cf-37b91500fb11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a6ba2d11eb675a97a609193582db6e006950a7374b673e166aa683b0f425db5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the Fashion-MNIST Dataset"
      ],
      "metadata": {
        "id": "dzz6U3Vpl8Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "gKyKHAvJmEaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e560beaf-f04c-4267-d541-90ee55aa5b4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "separating training(60,000 images) and testing(10,000) image data"
      ],
      "metadata": {
        "id": "5HdIEqNLprRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = dataset\n",
        "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255\n",
        "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255"
      ],
      "metadata": {
        "id": "_VViAXQ4mfc5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separating 10% from training data for validation"
      ],
      "metadata": {
        "id": "sA6OYRtpzjKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = int(len(X_train)*0.1)\n",
        "\n",
        "# randomly shuffle the indices of the data\n",
        "shuffled_indices = np.random.permutation(len(X_train))\n",
        "\n",
        "# split the shuffled data into training and validation sets\n",
        "train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]\n",
        "X_train, X_validation = X_train[train_indices], X_train[validation_indices]\n",
        "y_train, y_validation = y_train[train_indices], y_train[validation_indices]"
      ],
      "metadata": {
        "id": "BKkp-zpWwUna"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " plot 1 sample image for each class "
      ],
      "metadata": {
        "id": "UCyEEtXCqXAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"cs6910_trial\")\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
        "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "          \n",
        "no_of_classes = len(class_names)\n",
        "fig, axes = plt.subplots(1, no_of_classes, figsize=(20,20))\n",
        "\n",
        "list_of_images  = []   # to give to the wandb\n",
        "\n",
        "for i in range(no_of_classes):\n",
        "   \n",
        "    # Find the index of the first image of each class\n",
        "    idx = np.where(y_train == i)[0][0]\n",
        "    \n",
        "    # Plot the image\n",
        "    image = axes[i].imshow(X_train[idx].reshape(28,28), cmap='gray')\n",
        "    axes[i].set_title(class_names[i])\n",
        "    axes[i].axis('off')\n",
        "    list_of_images.append(image)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "yTyYtOi2skO4",
        "outputId": "aa93fd58-b717-4d20-8908-8d28423312a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230304_034014-8uid7coc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/haideraltaf/cs6910_trial/runs/8uid7coc' target=\"_blank\">grateful-darkness-2</a></strong> to <a href='https://wandb.ai/haideraltaf/cs6910_trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/haideraltaf/cs6910_trial' target=\"_blank\">https://wandb.ai/haideraltaf/cs6910_trial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/haideraltaf/cs6910_trial/runs/8uid7coc' target=\"_blank\">https://wandb.ai/haideraltaf/cs6910_trial/runs/8uid7coc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({\"Question 1\": [wandb.Image(img, caption=caption) for img, caption in zip(list_of_images, class_names)]})"
      ],
      "metadata": {
        "id": "98aiKUgYsZRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MWYcSsvqddp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 \n",
        "\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
      ],
      "metadata": {
        "id": "ZlUA82ekuTqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = int(input(\"Enter the number of Hidden + outer layer: \"))"
      ],
      "metadata": {
        "id": "_LP2q-Of4ApX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74848b50-6a95-41ab-a10b-703910eeb7b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of Hidden + outer layer: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
      ],
      "metadata": {
        "id": "H8IheuiD4Rv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed40eb3e-29da-4a3b-e2bb-0b8df1854e7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the numbers of neuron in each hidden layer: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_classes = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "3JFVNBN-4gB0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "choice = int(input(\"For random weights initialisation enter 1 and for xavier enter 2: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Y8casj40uH",
        "outputId": "9f7bb15b-007f-4cb3-dfce-ac2364fb664b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For random weights initialisation enter 1 and for xavier enter 2: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights(choice, n, L, X_train, no_of_classes):\n",
        "  \n",
        "  Weights = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if choice ==1:\n",
        "    temp = np.random.rand(n, len(X_train[0]))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.random.rand(n, n)\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.rand(no_of_classes, n)\n",
        "    Weights.append(temp)\n",
        "\n",
        "  if choice ==2:\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(n, len(X_train[0]))) \n",
        "    Weights.append(temp)\n",
        "    for i in range(1, L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n,n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes, n)) \n",
        "    Weights.append(temp)\n",
        "\n",
        "  if choice ==3:\n",
        "    temp = np.zeros((n, len(X_train[0])))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.zeros((n, n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.zeros((no_of_classes, n))\n",
        "    Weights.append(temp)\n",
        "\n",
        "  \n",
        "  return Weights\n",
        "\n"
      ],
      "metadata": {
        "id": "o4pDXaK65aNO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biases(choice, n, L, y_train, no_of_classes):\n",
        "  \n",
        "  bias = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if choice ==1:\n",
        "    for i in range(L-1):\n",
        "      temp = np.random.rand(n)  # for schochastic GD\n",
        "      #temp = np.random.rand(len(y_train),n)  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "    \n",
        "    temp = np.random.rand(no_of_classes)   # for schochastic GD\n",
        "    #temp = np.random.rand(len(y_train), no_of_classes)   # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if choice ==2:\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "    for i in range(L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n))   # for schochastic GD\n",
        "      #temp  = np.random.uniform(-limit, limit, size=(len(y_train),n))  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes))   # for schochastic GD\n",
        "    #temp = np.random.uniform(-limit, limit, size=(len(y_train),no_of_classes)) # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if choice ==3:\n",
        "    for i in range(L-1):\n",
        "      temp = np.zeros(n)\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.zeros(no_of_classes)\n",
        "    bias.append(temp)\n",
        "\n",
        "  \n",
        "  return bias\n"
      ],
      "metadata": {
        "id": "Mp2HVC8eUEvK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights = weights(choice, n, L, X_train, no_of_classes)"
      ],
      "metadata": {
        "id": "0Ki8Z7zdZE8g"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias = biases(choice, n, L, y_train, no_of_classes)"
      ],
      "metadata": {
        "id": "bNE8AXdi7fB1"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "SNe2k9o5fJCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(a):\n",
        "  a = np.float128(a)\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "def tanh(a):\n",
        "  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "\n",
        "def ReLu(a):\n",
        "  i=0\n",
        "  for ele in a:\n",
        "    a[i] = max(0,ele)\n",
        "    i+=1\n",
        "  if choice ==1:\n",
        "    return a - max(a)\n",
        "  #   j=0\n",
        "  #   for num in ele:\n",
        "\n",
        "  #     a[i][j] = max(0,num)\n",
        "  #     j+=1\n",
        "  #   i+=1\n",
        "  return a\n",
        "\n",
        "def softmax(a):\n",
        "  # temp = np.zeros_like(a)\n",
        "  # for i in range(len(temp)):\n",
        "  #   temp[i] = np.float128(a[i])\n",
        "  #exp_logits = np.exp(a)\n",
        "  return np.exp(a)/np.sum(np.exp(a))   \n",
        "  \n",
        "def der_sigmoid(a):\n",
        "  return sigmoid(a)*(1-sigmoid(a))\n",
        "\n",
        "def der_tanh(a):\n",
        "  return 1-(tanh(a)*tanh(a))\n",
        "\n",
        "def der_ReLu(a):\n",
        "\n",
        "  # it will create a matrix of same dimension as of a.\n",
        "  gradient = np.zeros_like(a)  \n",
        "  # sets the entries of gradient to 1 where the corresponding entries of x>=0\n",
        "  gradient[a >= 0] = 1\n",
        "  return gradient"
      ],
      "metadata": {
        "id": "zQeYcLR3eZXr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions"
      ],
      "metadata": {
        "id": "lVMnOZ5LI-Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_dash, y_train, X_train):\n",
        "  losses = -np.log(y_dash[y_train])/len(X_train)\n",
        "  return losses\n",
        "\n",
        "def MSE_loss(y_dash, y_train, X_train):\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "  losses = (np.sum(y_dash - y_train_modified))\n",
        "  return losses"
      ],
      "metadata": {
        "id": "7D-NSGVpI9jt"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation"
      ],
      "metadata": {
        "id": "MRsgs5EJypw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(Weights, bias, x_input, L):\n",
        "  \n",
        "  h = x_input\n",
        "  a_out = []\n",
        "  h_out = []\n",
        "  h_out.append(h)\n",
        "  \n",
        "  ## for hidden layers\n",
        "  for k in range(L-1):\n",
        "    #a = np.dot(h, Weights[k]) + bias[k]\n",
        "    a = np.matmul(Weights[k], h) + bias[k]\n",
        "    a_out.append(a)\n",
        "    ## default activation function is sigmoid \n",
        "    h = sigmoid(a)\n",
        "    h_out.append(h)\n",
        "\n",
        "  ## In outer layer softmax function\n",
        "  #a = np.dot(h, Weights[L-1]) + bias[L-1]\n",
        "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
        "  a_out.append(a)\n",
        "  y_dash = softmax(a)\n",
        "  \n",
        "\n",
        "  return a_out, h_out, y_dash\n"
      ],
      "metadata": {
        "id": "jOypoQjEt81D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward propagation"
      ],
      "metadata": {
        "id": "z2DZ8Ewi05Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(a_out, h_out, y_train, y_dash, Weights, L, L2_decay):\n",
        "\n",
        "  grad_W = [0]*L\n",
        "  grad_b = [0]*L\n",
        "\n",
        "  \n",
        "  ## change each y_train into an array of 10 values\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "    \n",
        "  output_gradient = -(y_train_modified - y_dash)\n",
        "\n",
        "  for k in range(L, 0, -1):\n",
        "\n",
        "    ## compute gradients w.r.t parameters\n",
        "    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), h_out[k-1].reshape(1,len(h_out[k-1]))) \n",
        "    grad_W[k-1] = W_gradient\n",
        "\n",
        "    b_gradients = output_gradient \n",
        "    grad_b[k-1] = b_gradients\n",
        "   \n",
        "    if k==1:\n",
        "      continue\n",
        "    ## compute gradients w.r.t layer below\n",
        "    weight = Weights[k-1]\n",
        "    h_gradient = np.matmul(weight.T, output_gradient)\n",
        "\n",
        "    ## compute the gradient of pre activation layer\n",
        "    output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))\n",
        "\n",
        "  return grad_W, grad_b\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6s4dnd6ykc4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_out, h_out, y_dash = forward_propagation(Weights, bias, X_train[0], L)"
      ],
      "metadata": {
        "id": "gbH-Ru76krq7"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights, L, 0.005)"
      ],
      "metadata": {
        "id": "jfoB0bBwkGTi"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy"
      ],
      "metadata": {
        "id": "7Jf_j_EtFJhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_accuracy(X_validation, y_validation, Weights, bias, L):\n",
        "\n",
        "  y_pred = np.zeros((len(X_validation), 10))\n",
        "  i=0\n",
        "  for x in X_validation:\n",
        "    _, _, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "    y_pred[i] = y_dash\n",
        "    i+=1\n",
        "\n",
        "  correct = 0\n",
        "  for array,y in zip(y_pred, y_validation):\n",
        "    if np.argmax(array)==y:\n",
        "      correct+=1\n",
        "  accuracy = correct*100/len(y_validation)\n",
        "\n",
        "  return  accuracy"
      ],
      "metadata": {
        "id": "4vQxChQuUQDP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, L)"
      ],
      "metadata": {
        "id": "picBbO9RYcK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abb715b-44d4-489b-f221-9b50de06ac18"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.3"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Gradient Descent \n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent.\n",
        "if batch_size = Number of samples, the algorithm will be vanilla gradient descent\n"
      ],
      "metadata": {
        "id": "KsS21O64Kvzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_gradient_descent(learning_rate, Weights, bias, L, y_train, X_train, no_of_classes, batch_size, L2_decay):\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x,y in zip(X_train,y_train):\n",
        "\n",
        "    ##x,y = np.float128(x), np.float128(y)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "    \n",
        "    #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    loss += loss_iter\n",
        "\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L, L2_decay)\n",
        "\n",
        "    ## Adding the gradients of weights and biases\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "     \n",
        "    num_points_seen+=1\n",
        "    \n",
        "    if num_points_seen%batch_size == 0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "      # Weights updates\n",
        "      \n",
        "      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      \n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  # Adding L2 regularization loss after an epochS\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 2*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "  \n",
        "  return Weights, bias, loss\n"
      ],
      "metadata": {
        "id": "lF615oFSKuv-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  Weights, bias, loss = Minibatch_gradient_descent(0.0001, Weights, bias, L, y_train, X_train, no_of_classes, 25, 0)\n",
        "  print(i, loss)\n",
        "# Finish the WandB run\n",
        "#wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O11C1AEYPgFI",
        "outputId": "e0dcc1b4-c6da-4d90-ca07-104cfb22a9f3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.7502577582561744491\n",
            "1 1.8043595735334898072\n",
            "2 1.5832133233969766883\n",
            "3 1.4359097059015334201\n",
            "4 1.3266704617557013966\n",
            "5 1.2404861694744863456\n",
            "6 1.1697466090772752014\n",
            "7 1.1106282530330283914\n",
            "8 1.0608271915901231603\n",
            "9 1.0185881267326671076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, L)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F60POmeeW-51",
        "outputId": "8b3cd158-e71f-487d-cab7-13e6cac36897"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.56666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Momentum based Gradient Descent\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "IK6JEr8ORuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_Momentum_GD(Weights, bias, L, X_train, y_train, beta, no_of_classes, prev_uw, prev_ub, learning_rate, batch_size, L2_decay):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "    ## initialize the loss\n",
        "    loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x,y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "      loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "      loss += loss_iter\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L, L2_decay)\n",
        "\n",
        "      ## Adding the gradients of weights and biases\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        " \n",
        "      if num_points_seen%batch_size==0:\n",
        "\n",
        "        ## Add L2 regularization penalty to gradient\n",
        "        dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "\n",
        "        ## momentum based wight updates\n",
        "        uw = [prev_uw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "        ub = [prev_ub[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "        \n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - uw[i] for i in range(len(uw))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - ub[i] for i in range(len(ub))]\n",
        "\n",
        "        # assign present to the history \n",
        "        prev_uw = uw\n",
        "        prev_ub = ub\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "    \n",
        "     # Adding L2 regularization loss\n",
        "    for i in range(len(Weights)):\n",
        "      loss = loss + 2*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "\n",
        "    return  Weights, bias, loss"
      ],
      "metadata": {
        "id": "N9hSwUCaRVVz"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## initializing the initial value for momentum(zero)\n",
        "prev_uw = weights(3, n, L, X_train, no_of_classes)\n",
        "prev_ub = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "\n",
        "  Weights, bias, loss = Minibatch_Momentum_GD(Weights, bias, L, X_train, y_train, 0.9, no_of_classes, prev_uw, prev_ub, 0.0001, 25, 0.0005)\n",
        "  print(i, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-rcveBQD_w",
        "outputId": "4797adf0-a83f-4d32-a5ea-4b37913b6caa"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.5466685496257547951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_train, y_train, Weights, bias, L)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afKsmhTauV2e",
        "outputId": "af27c13c-4354-47c1-b394-6d37d73026e9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.64814814814815"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nesterov Accelerated Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "KcBoS2WQeF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_NAG(Weights, bias, L, X_train, y_train, beta, no_of_classes, prev_vw, prev_vb, learning_rate, batch_size):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    # do partial updates\n",
        "    v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]\n",
        "    v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]\n",
        "      bias    = [bias[i]-v_b[i] for i in range(len(bias))]\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "\n",
        "      ## Look Ahead\n",
        "      ## Adding the gradients of weights and biases\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        ## momentum based wight updates\n",
        "        vw = [prev_vw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "        vb = [prev_vb[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "\n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - vw[i] for i in range(len(vw))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - vb[i] for i in range(len(vb))]\n",
        "\n",
        "        # assign present to the history \n",
        "        prev_uw = vw\n",
        "        prev_ub = vb\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "EhjA6UuceE_v"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## initializing the initial value for momentum(zero)\n",
        "prev_vw = weights(3, n, L, X_train, no_of_classes)\n",
        "prev_vb = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_NAG(Weights, bias, L, X_train, y_train, 0.9, no_of_classes, prev_vw, prev_vb, 0.0001, 25)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "id": "yFL7QpxmVKoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Gradient(AdaGrad) based Gradient Descent- Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "ti8ADwESXAGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_AdaGrad(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, learning_rate, batch_size):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]\n",
        "        v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]\n",
        "\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "ktWLOxF2WnnD"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MiniBatch AdaGrad\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_AdaGrad(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, v_w, v_b, 0.0001, 50)\n",
        "  print(i,cross_entropy_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmF6kRdHX6-v",
        "outputId": "0cb6ef26-51b2-42f1-f922-b50d1d5be77a"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 50508.469899747501614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Root Mean Squared Propagation(RMSProp) Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "Qwy4m-glvWKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_RMSProp(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, learning_rate, batch_size, beta):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "      \n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "        v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "-HH3ZkpwvcJS"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MiniBatch RMSProp\n",
        "%%time\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_RMSProp(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, v_w, v_b, 0.0001, 25, 0.9)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NjTeTrU1Nbv",
        "outputId": "fcff3885-39a9-44fa-a0a3-317a3f3cd790"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 46139.93685923521568\n",
            "CPU times: user 34.5 s, sys: 72.8 ms, total: 34.6 s\n",
            "Wall time: 35.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Delta(AdaDelta) gradient descent - Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "6CLGHkDl4t5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_AdaDelta(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, u_w, u_b, learning_rate, batch_size, beta):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "      \n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "        v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "        del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]\n",
        "        del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]\n",
        "\n",
        "        u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]\n",
        "        u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]\n",
        "\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - del_b[i] for i in range(len(del_b))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "FAnqIPVXZj9q"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## MiniBatch AdaDelta\n",
        "u_w = weights(3, n, L, X_train, no_of_classes)\n",
        "u_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(10):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_AdaDelta(Weights, bias, L, X_train, y_train, 1E-8, no_of_classes, v_w, v_b, u_w, u_b, 0.001, 25, 0.9)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "iTGhRTO5aBdv",
        "outputId": "346559d8-f80d-4654-e0d3-f6f9e0c30830"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 38616.291048687305704\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-432b9b032153>\u001b[0m in \u001b[0;36mMinibatch_AdaDelta\u001b[0;34m(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, u_w, u_b, learning_rate, batch_size, beta)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-432b9b032153>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive moments(Adam) Gradient Descent- MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "BklUlyYNE2V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_Adam(Weights, bias, L, X_train, y_train, eps, no_of_classes, learning_rate, batch_size, beta1, beta2, eta, max_epoch):\n",
        "\n",
        "  m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    \n",
        "  for epoch in range(max_epoch):\n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "      ## initialize the loss\n",
        "      cross_entropy_loss = 0\n",
        "      num_points_seen = 0\n",
        "\n",
        "      for x, y in zip(X_train, y_train):\n",
        "\n",
        "        #x = np.float128(x)\n",
        "\n",
        "        ## Forward propagation\n",
        "        a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "        \n",
        "        cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "        ## Backward Propagation\n",
        "        grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "        \n",
        "        dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "        dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "        num_points_seen +=1\n",
        "\n",
        "        if num_points_seen%batch_size==0:\n",
        "          #compute intermediate values\n",
        "\n",
        "          m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "          m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "          v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "          v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "          m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "          m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "          v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "          v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "          # Weights updates\n",
        "          Weights = [Weights[i] - eta*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "          # Biases updates\n",
        "          bias = [bias[i] - eta*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "          dW = weights(3, n, L, X_train, no_of_classes)\n",
        "          dB = biases(3, n, L, y_train, no_of_classes)\n",
        "      \n",
        "      print(epoch, cross_entropy_loss)\n",
        "\n",
        "  return Weights, bias\n"
      ],
      "metadata": {
        "id": "pCvLuniEE1Z_"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Minibatch_Adam(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, 0.0001, 25, 0.9, 0.999, 0.1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kaWlRUbkeTw",
        "outputId": "ec331570-125a-4752-f880-4865baa3d564"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 54748.154292107247223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RD_lhshLj0Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NAG + Adam = NAdam Gradient descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "nGmz0pRpBPaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_NAdam(Weights, bias, L, X_train, y_train, eps, no_of_classes, batch_size, beta1, beta2, eta, max_epoch):\n",
        "\n",
        "  m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    \n",
        "  for epoch in range(max_epoch):\n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "      ## initialize the loss\n",
        "      cross_entropy_loss = 0\n",
        "      num_points_seen = 0\n",
        "\n",
        "      for x, y in zip(X_train, y_train):\n",
        "\n",
        "        #x = np.float128(x)\n",
        "\n",
        "        ## Forward propagation\n",
        "        a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "        \n",
        "        cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "        ## Backward Propagation\n",
        "        grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "        \n",
        "        dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "        dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "        num_points_seen +=1\n",
        "\n",
        "        if num_points_seen%batch_size==0:\n",
        "          #compute intermediate values\n",
        "\n",
        "          m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "          m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "          v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "          v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "          m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "          m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "          v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "          v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "          # Weights updates\n",
        "          Weights = [Weights[i] - (eta/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "          # Biases updates\n",
        "          bias = [bias[i] - (eta/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "          dW = weights(3, n, L, X_train, no_of_classes)\n",
        "          dB = biases(3, n, L, y_train, no_of_classes)\n",
        "      \n",
        "      print(epoch, cross_entropy_loss)\n",
        "\n",
        "  return Weights, bias\n"
      ],
      "metadata": {
        "id": "GLRgSWWOGSeZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Minibatch_NAdam(Weights, bias, L, X_train, y_train, 1e-10, no_of_classes, 25, 0.9, 0.999, 0.1, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYpytPVUn4PZ",
        "outputId": "9d681941-1f41-4899-8ddb-ae67255a4677"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 84607.72218064582286\n",
            "1 65691.58314440987447\n",
            "2 63625.163485619256303\n",
            "3 61345.835480034633015\n",
            "4 56382.25314378042691\n",
            "5 54782.47692052668505\n",
            "6 49958.795616479803282\n",
            "7 43365.806069021878436\n",
            "8 42394.361408850225175\n",
            "9 41293.58527524336861\n"
          ]
        }
      ]
    }
  ]
}