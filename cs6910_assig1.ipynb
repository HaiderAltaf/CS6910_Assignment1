{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKf3MMJvnNUZKrQ2xdCCtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiderAltaf/Gradient-Descent-Algorithm-and-its-variants/blob/main/cs6910_assig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ],
      "metadata": {
        "id": "-5cMQpFel2pj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O_-ut5eWlniQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "id": "OZ4uiwM-V1O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"cs6910-assignment_1\")"
      ],
      "metadata": {
        "id": "UG7KjcpKWBfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the Fashion-MNIST Dataset"
      ],
      "metadata": {
        "id": "dzz6U3Vpl8Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "gKyKHAvJmEaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b0c0ff-edf7-4abd-8741-aa6ee45d2a17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "separating training(60,000 images) and testing(10,000) image data"
      ],
      "metadata": {
        "id": "5HdIEqNLprRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = dataset\n",
        "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255\n",
        "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255"
      ],
      "metadata": {
        "id": "_VViAXQ4mfc5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separating 10% from training data for validation"
      ],
      "metadata": {
        "id": "sA6OYRtpzjKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = int(len(X_train)*0.1)\n",
        "\n",
        "# randomly shuffle the indices of the data\n",
        "shuffled_indices = np.random.permutation(len(X_train))\n",
        "\n",
        "# split the shuffled data into training and validation sets\n",
        "train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]\n",
        "X_train, X_validation = X_train[train_indices], X_train[validation_indices]\n",
        "y_train, y_validation = y_train[train_indices], y_train[validation_indices]"
      ],
      "metadata": {
        "id": "BKkp-zpWwUna"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of classes in data"
      ],
      "metadata": {
        "id": "O2t1ke4Z5Mvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " plot 1 sample image for each class "
      ],
      "metadata": {
        "id": "UCyEEtXCqXAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
        "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "          \n",
        "no_of_classes = len(class_names)\n",
        "fig, axes = plt.subplots(1, no_of_classes, figsize=(20,20))\n",
        "\n",
        "for i in range(no_of_classes):\n",
        "\n",
        "    # Find the index of the first image of each class\n",
        "    idx = np.where(y_train == i)[0][0]\n",
        "    \n",
        "    # Plot the image\n",
        "    axes[i].imshow(x_train[idx].reshape(28,28), cmap='gray')\n",
        "    axes[i].set_title(class_names[i])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "yTyYtOi2skO4",
        "outputId": "9f8a471e-993b-48be-c1c2-10b0fa093a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MWYcSsvqddp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 \n",
        "\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
      ],
      "metadata": {
        "id": "ZlUA82ekuTqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = int(input(\"Enter the number of Hidden + outer layer: \"))"
      ],
      "metadata": {
        "id": "_LP2q-Of4ApX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74848b50-6a95-41ab-a10b-703910eeb7b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of Hidden + outer layer: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
      ],
      "metadata": {
        "id": "H8IheuiD4Rv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed40eb3e-29da-4a3b-e2bb-0b8df1854e7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the numbers of neuron in each hidden layer: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_classes = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "3JFVNBN-4gB0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "choice = int(input(\"For random weights initialisation enter 1 and for xavier enter 2: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Y8casj40uH",
        "outputId": "9f7bb15b-007f-4cb3-dfce-ac2364fb664b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For random weights initialisation enter 1 and for xavier enter 2: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights(choice, n, L, X_train, no_of_classes):\n",
        "  \n",
        "  Weights = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if choice ==1:\n",
        "    temp = np.random.rand(n, len(X_train[0]))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.random.rand(n, n)\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.rand(no_of_classes, n)\n",
        "    Weights.append(temp)\n",
        "\n",
        "  if choice ==2:\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(n, len(X_train[0]))) \n",
        "    Weights.append(temp)\n",
        "    for i in range(1, L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n,n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes, n)) \n",
        "    Weights.append(temp)\n",
        "\n",
        "  if choice ==3:\n",
        "    temp = np.zeros((n, len(X_train[0])))\n",
        "    Weights.append(temp)\n",
        "\n",
        "    for i in range(1, L-1):\n",
        "      temp = np.zeros((n, n))\n",
        "      Weights.append(temp)\n",
        "\n",
        "    temp = np.zeros((no_of_classes, n))\n",
        "    Weights.append(temp)\n",
        "\n",
        "  \n",
        "  return Weights\n",
        "\n"
      ],
      "metadata": {
        "id": "o4pDXaK65aNO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biases(choice, n, L, y_train, no_of_classes):\n",
        "  \n",
        "  bias = []\n",
        "  np.random.seed(0)\n",
        "\n",
        "  if choice ==1:\n",
        "    for i in range(L-1):\n",
        "      temp = np.random.rand(n)  # for schochastic GD\n",
        "      #temp = np.random.rand(len(y_train),n)  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "    \n",
        "    temp = np.random.rand(no_of_classes)   # for schochastic GD\n",
        "    #temp = np.random.rand(len(y_train), no_of_classes)   # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if choice ==2:\n",
        "    scale = 1/max(1, (2+2)/2 )\n",
        "    limit = math.sqrt(3*scale)\n",
        "    for i in range(L-1):\n",
        "      temp  = np.random.uniform(-limit, limit, size=(n))   # for schochastic GD\n",
        "      #temp  = np.random.uniform(-limit, limit, size=(len(y_train),n))  # for Vanilla GD\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.random.uniform(-limit, limit, size=(no_of_classes))   # for schochastic GD\n",
        "    #temp = np.random.uniform(-limit, limit, size=(len(y_train),no_of_classes)) # for Vanilla GD\n",
        "    bias.append(temp)\n",
        "\n",
        "  if choice ==3:\n",
        "    for i in range(L-1):\n",
        "      temp = np.zeros(n)\n",
        "      bias.append(temp)\n",
        "\n",
        "    temp = np.zeros(no_of_classes)\n",
        "    bias.append(temp)\n",
        "\n",
        "  \n",
        "  return bias\n"
      ],
      "metadata": {
        "id": "Mp2HVC8eUEvK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights = weights(choice, n, L, X_train, no_of_classes)"
      ],
      "metadata": {
        "id": "0Ki8Z7zdZE8g"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias = biases(choice, n, L, y_train, no_of_classes)"
      ],
      "metadata": {
        "id": "bNE8AXdi7fB1"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "SNe2k9o5fJCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(a):\n",
        "  a = np.float128(a)\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "def tanh(a):\n",
        "  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "\n",
        "def ReLu(a):\n",
        "  i=0\n",
        "  for ele in a:\n",
        "    a[i] = max(0,ele)\n",
        "    i+=1\n",
        "  if choice ==1:\n",
        "    return a - max(a)\n",
        "  #   j=0\n",
        "  #   for num in ele:\n",
        "\n",
        "  #     a[i][j] = max(0,num)\n",
        "  #     j+=1\n",
        "  #   i+=1\n",
        "  return a\n",
        "\n",
        "def softmax(a):\n",
        "  # temp = np.zeros_like(a)\n",
        "  # for i in range(len(temp)):\n",
        "  #   temp[i] = np.float128(a[i])\n",
        "  #exp_logits = np.exp(a)\n",
        "  return np.exp(a)/np.sum(np.exp(a))   \n",
        "  \n",
        "def der_sigmoid(a):\n",
        "  return sigmoid(a)*(1-sigmoid(a))\n",
        "\n",
        "def der_tanh(a):\n",
        "  return 1-(tanh(a)*tanh(a))\n",
        "\n",
        "def der_ReLu(a):\n",
        "\n",
        "  # it will create a matrix of same dimension as of a.\n",
        "  gradient = np.zeros_like(a)  \n",
        "  # sets the entries of gradient to 1 where the corresponding entries of x>=0\n",
        "  gradient[a >= 0] = 1\n",
        "  return gradient"
      ],
      "metadata": {
        "id": "zQeYcLR3eZXr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions"
      ],
      "metadata": {
        "id": "lVMnOZ5LI-Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_dash, y_train, X_train):\n",
        "  losses = -np.log(y_dash[y_train])/len(X_train)\n",
        "  return losses\n",
        "\n",
        "def MSE_loss(y_dash, y_train, X_train):\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "  losses = (np.sum(y_dash - y_train_modified))\n",
        "  return losses"
      ],
      "metadata": {
        "id": "7D-NSGVpI9jt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation"
      ],
      "metadata": {
        "id": "MRsgs5EJypw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(Weights, bias, x_input, L):\n",
        "  \n",
        "  h = x_input\n",
        "  a_out = []\n",
        "  h_out = []\n",
        "  h_out.append(h)\n",
        "  \n",
        "  ## for hidden layers\n",
        "  for k in range(L-1):\n",
        "    #a = np.dot(h, Weights[k]) + bias[k]\n",
        "    a = np.matmul(Weights[k], h) + bias[k]\n",
        "    a_out.append(a)\n",
        "    ## default activation function is sigmoid \n",
        "    h = sigmoid(a)\n",
        "    h_out.append(h)\n",
        "\n",
        "  ## In outer layer softmax function\n",
        "  #a = np.dot(h, Weights[L-1]) + bias[L-1]\n",
        "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
        "  a_out.append(a)\n",
        "  y_dash = softmax(a)\n",
        "  \n",
        "\n",
        "  return a_out, h_out, y_dash\n"
      ],
      "metadata": {
        "id": "jOypoQjEt81D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward propagation"
      ],
      "metadata": {
        "id": "z2DZ8Ewi05Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(a_out, h_out, y_train, y_dash, Weights, L, L2_decay):\n",
        "\n",
        "  grad_W = [0]*L\n",
        "  grad_b = [0]*L\n",
        "\n",
        "  \n",
        "  ## change each y_train into an array of 10 values\n",
        "  y_train_modified = np.zeros(10)\n",
        "  y_train_modified[y_train] = 1\n",
        "    \n",
        "  output_gradient = -(y_train_modified - y_dash)\n",
        "\n",
        "  for k in range(L, 0, -1):\n",
        "\n",
        "    ## compute gradients w.r.t parameters\n",
        "    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), h_out[k-1].reshape(1,len(h_out[k-1]))) \n",
        "    grad_W[k-1] = W_gradient\n",
        "\n",
        "    b_gradients = output_gradient \n",
        "    grad_b[k-1] = b_gradients\n",
        "   \n",
        "    if k==1:\n",
        "      continue\n",
        "    ## compute gradients w.r.t layer below\n",
        "    weight = Weights[k-1]\n",
        "    h_gradient = np.matmul(weight.T, output_gradient)\n",
        "\n",
        "    ## compute the gradient of pre activation layer\n",
        "    output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))\n",
        "\n",
        "  return grad_W, grad_b\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6s4dnd6ykc4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_out, h_out, y_dash = forward_propagation(Weights, bias, X_train[0], L)"
      ],
      "metadata": {
        "id": "gbH-Ru76krq7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_W, grad_b = backward_propagation(a_out, h_out, y_train, y_dash, Weights, L, 0.005)"
      ],
      "metadata": {
        "id": "jfoB0bBwkGTi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy"
      ],
      "metadata": {
        "id": "7Jf_j_EtFJhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_accuracy(X_validation, y_validation, Weights, bias, L):\n",
        "\n",
        "  y_pred = np.zeros((len(X_validation), 10))\n",
        "  i=0\n",
        "  for x in X_validation:\n",
        "    _, _, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "    y_pred[i] = y_dash\n",
        "    i+=1\n",
        "\n",
        "  correct = 0\n",
        "  for array,y in zip(y_pred, y_validation):\n",
        "    if np.argmax(array)==y:\n",
        "      correct+=1\n",
        "  accuracy = correct*100/len(y_validation)\n",
        "\n",
        "  return  accuracy"
      ],
      "metadata": {
        "id": "4vQxChQuUQDP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, L)"
      ],
      "metadata": {
        "id": "picBbO9RYcK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abb715b-44d4-489b-f221-9b50de06ac18"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.3"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Gradient Descent \n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent.\n",
        "if batch_size = Number of samples, the algorithm will be vanilla gradient descent\n"
      ],
      "metadata": {
        "id": "KsS21O64Kvzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_gradient_descent(learning_rate, Weights, bias, L, y_train, X_train, no_of_classes, batch_size, L2_decay):\n",
        "  \n",
        "  ## initialize the gradients of weights and biases\n",
        "  dW = weights(3, n, L, X_train, no_of_classes)\n",
        "  dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  ## initialize the loss\n",
        "  loss = 0\n",
        "  num_points_seen = 0\n",
        "\n",
        "  for x,y in zip(X_train,y_train):\n",
        "\n",
        "    ##x,y = np.float128(x), np.float128(y)\n",
        "\n",
        "    ## Forward propagation\n",
        "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "    \n",
        "    #loss_iter = MSE_loss(y_dash, y_train, X_train)  # for MSE loss use this\n",
        "    loss_iter = cross_entropy_loss(y_dash, y, X_train)  # for cross entropy use this\n",
        "    loss += loss_iter\n",
        "\n",
        "\n",
        "    ## Backward Propagation\n",
        "    grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L, L2_decay)\n",
        "\n",
        "    ## Adding the gradients of weights and biases\n",
        "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "     \n",
        "    num_points_seen+=1\n",
        "    \n",
        "    if num_points_seen%batch_size == 0:\n",
        "\n",
        "      ## Add L2 regularization penalty to gradient\n",
        "      dW = [dW[i] + 2*L2_decay*Weights[i]/len(X_train) for i in range(len(dW))]\n",
        "      # Weights updates\n",
        "      \n",
        "      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      # Biases updates\n",
        "      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]\n",
        "\n",
        "      \n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "  # Adding L2 regularization loss\n",
        "  for i in range(len(Weights)):\n",
        "    loss = loss + 2*L2_decay*np.sum(Weights[i]**2)/len(X_train)\n",
        "  \n",
        "  return Weights, bias, loss\n"
      ],
      "metadata": {
        "id": "lF615oFSKuv-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  Weights, bias, loss = Minibatch_gradient_descent(0.0001, Weights, bias, L, y_train, X_train, no_of_classes, 25, 0)\n",
        "  print(i, loss)\n",
        "# Finish the WandB run\n",
        "#wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O11C1AEYPgFI",
        "outputId": "e0dcc1b4-c6da-4d90-ca07-104cfb22a9f3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.7502577582561744491\n",
            "1 1.8043595735334898072\n",
            "2 1.5832133233969766883\n",
            "3 1.4359097059015334201\n",
            "4 1.3266704617557013966\n",
            "5 1.2404861694744863456\n",
            "6 1.1697466090772752014\n",
            "7 1.1106282530330283914\n",
            "8 1.0608271915901231603\n",
            "9 1.0185881267326671076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_accuracy(X_validation, y_validation, Weights, bias, L)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F60POmeeW-51",
        "outputId": "797eee88-f9b9-48e1-dc5d-b0049f86144d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.53333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini Batch Momentum based Gradient Descent\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "IK6JEr8ORuE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_Momentum_GD(Weights, bias, L, X_train, y_train, beta, no_of_classes, prev_uw, prev_ub, learning_rate, batch_size, L2_decay):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x,y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "    \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L, L2_decay)\n",
        "\n",
        "      ## Adding the gradients of weights and biases\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        " \n",
        "      if num_points_seen%batch_size==0:\n",
        "        ## momentum based wight updates\n",
        "        uw = [prev_uw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "        ub = [prev_ub[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "        \n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - uw[i] for i in range(len(uw))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - ub[i] for i in range(len(ub))]\n",
        "\n",
        "        # assign present to the history \n",
        "        prev_uw = uw\n",
        "        prev_ub = ub\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "N9hSwUCaRVVz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## initializing the initial value for momentum(zero)\n",
        "prev_uw = weights(3, n, L, X_train, no_of_classes)\n",
        "prev_ub = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(9):\n",
        "\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_Momentum_GD(Weights, bias, L, X_train, y_train, 0.9, no_of_classes, prev_uw, prev_ub, 0.0001, 25, 0.005)\n",
        "  print(i, cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-rcveBQD_w",
        "outputId": "9a11e2ee-b3a6-4be2-f4b8-99349f95592e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 79670.345872985654104\n",
            "1 47785.15090954947926\n",
            "2 40630.868163315778204\n",
            "3 37150.6177041099439\n",
            "4 34943.459926445599734\n",
            "5 33358.25538093008927\n",
            "6 32137.919226511332589\n",
            "7 31153.441210135198082\n",
            "8 30331.900578843719494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nesterov Accelerated Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "KcBoS2WQeF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_NAG(Weights, bias, L, X_train, y_train, beta, no_of_classes, prev_vw, prev_vb, learning_rate, batch_size):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    # do partial updates\n",
        "    v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]\n",
        "    v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]\n",
        "      bias    = [bias[i]-v_b[i] for i in range(len(bias))]\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "\n",
        "      ## Look Ahead\n",
        "      ## Adding the gradients of weights and biases\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        ## momentum based wight updates\n",
        "        vw = [prev_vw[i]*beta + dW[i]*learning_rate for i in range(len(dW))]\n",
        "        vb = [prev_vb[i]*beta + dB[i]*learning_rate for i in range(len(dB))]\n",
        "\n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - vw[i] for i in range(len(vw))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - vb[i] for i in range(len(vb))]\n",
        "\n",
        "        # assign present to the history \n",
        "        prev_uw = vw\n",
        "        prev_ub = vb\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "EhjA6UuceE_v"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## initializing the initial value for momentum(zero)\n",
        "prev_vw = weights(3, n, L, X_train, no_of_classes)\n",
        "prev_vb = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_NAG(Weights, bias, L, X_train, y_train, 0.9, no_of_classes, prev_vw, prev_vb, 0.0001, 25)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "id": "yFL7QpxmVKoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Gradient(AdaGrad) based Gradient Descent- Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "ti8ADwESXAGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_AdaGrad(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, learning_rate, batch_size):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "\n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]\n",
        "        v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]\n",
        "\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "ktWLOxF2WnnD"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MiniBatch AdaGrad\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_AdaGrad(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, v_w, v_b, 0.0001, 50)\n",
        "  print(i,cross_entropy_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmF6kRdHX6-v",
        "outputId": "0cb6ef26-51b2-42f1-f922-b50d1d5be77a"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 50508.469899747501614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Root Mean Squared Propagation(RMSProp) Gradient Descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "Qwy4m-glvWKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_RMSProp(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, learning_rate, batch_size, beta):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "      \n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "        v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "        ## Weights and biases updates\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "-HH3ZkpwvcJS"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MiniBatch RMSProp\n",
        "%%time\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(1):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_RMSProp(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, v_w, v_b, 0.0001, 25, 0.9)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NjTeTrU1Nbv",
        "outputId": "fcff3885-39a9-44fa-a0a3-317a3f3cd790"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 46139.93685923521568\n",
            "CPU times: user 34.5 s, sys: 72.8 ms, total: 34.6 s\n",
            "Wall time: 35.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive Delta(AdaDelta) gradient descent - Minibatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "6CLGHkDl4t5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_AdaDelta(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, u_w, u_b, learning_rate, batch_size, beta):\n",
        "\n",
        "    ## initialize the gradients of weights and biases\n",
        "    dW = weights(3, n, L, X_train, no_of_classes)\n",
        "    dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    ## initialize the loss\n",
        "    cross_entropy_loss = 0\n",
        "    num_points_seen = 0\n",
        "\n",
        "    for x, y in zip(X_train, y_train):\n",
        "\n",
        "      #x = np.float128(x)\n",
        "\n",
        "      ## Forward propagation\n",
        "      a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "      \n",
        "      cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "      ## Backward Propagation\n",
        "      grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "      \n",
        "      dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "      dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "      num_points_seen +=1\n",
        "\n",
        "      if num_points_seen%batch_size==0:\n",
        "        #compute intermediate values\n",
        "        v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
        "        v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
        "\n",
        "        del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]\n",
        "        del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]\n",
        "\n",
        "        u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]\n",
        "        u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]\n",
        "\n",
        "        # Weights updates\n",
        "        Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]\n",
        "\n",
        "        # Biases updates\n",
        "        bias = [bias[i] - del_b[i] for i in range(len(del_b))]\n",
        "\n",
        "        dW = weights(3, n, L, X_train, no_of_classes)\n",
        "        dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "\n",
        "    return  Weights, bias, cross_entropy_loss"
      ],
      "metadata": {
        "id": "FAnqIPVXZj9q"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## MiniBatch AdaDelta\n",
        "u_w = weights(3, n, L, X_train, no_of_classes)\n",
        "u_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "for i in range(10):\n",
        "  Weights, bias, cross_entropy_loss = Minibatch_AdaDelta(Weights, bias, L, X_train, y_train, 1E-8, no_of_classes, v_w, v_b, u_w, u_b, 0.001, 25, 0.9)\n",
        "  print(i,cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "iTGhRTO5aBdv",
        "outputId": "346559d8-f80d-4654-e0d3-f6f9e0c30830"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 38616.291048687305704\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-432b9b032153>\u001b[0m in \u001b[0;36mMinibatch_AdaDelta\u001b[0;34m(Weights, bias, L, X_train, y_train, eps, no_of_classes, v_w, v_b, u_w, u_b, learning_rate, batch_size, beta)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-432b9b032153>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive moments(Adam) Gradient Descent- MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "BklUlyYNE2V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_Adam(Weights, bias, L, X_train, y_train, eps, no_of_classes, learning_rate, batch_size, beta1, beta2, eta, max_epoch):\n",
        "\n",
        "  m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    \n",
        "  for epoch in range(max_epoch):\n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "      ## initialize the loss\n",
        "      cross_entropy_loss = 0\n",
        "      num_points_seen = 0\n",
        "\n",
        "      for x, y in zip(X_train, y_train):\n",
        "\n",
        "        #x = np.float128(x)\n",
        "\n",
        "        ## Forward propagation\n",
        "        a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "        \n",
        "        cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "        ## Backward Propagation\n",
        "        grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "        \n",
        "        dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "        dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "        num_points_seen +=1\n",
        "\n",
        "        if num_points_seen%batch_size==0:\n",
        "          #compute intermediate values\n",
        "\n",
        "          m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "          m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "          v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "          v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "          m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "          m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "          v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "          v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "          # Weights updates\n",
        "          Weights = [Weights[i] - eta*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "          # Biases updates\n",
        "          bias = [bias[i] - eta*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]\n",
        "\n",
        "          dW = weights(3, n, L, X_train, no_of_classes)\n",
        "          dB = biases(3, n, L, y_train, no_of_classes)\n",
        "      \n",
        "      print(epoch, cross_entropy_loss)\n",
        "\n",
        "  return Weights, bias\n"
      ],
      "metadata": {
        "id": "pCvLuniEE1Z_"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Minibatch_Adam(Weights, bias, L, X_train, y_train, 1e-8, no_of_classes, 0.0001, 25, 0.9, 0.999, 0.1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kaWlRUbkeTw",
        "outputId": "ec331570-125a-4752-f880-4865baa3d564"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 54748.154292107247223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RD_lhshLj0Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NAG + Adam = NAdam Gradient descent - MiniBatch\n",
        "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
      ],
      "metadata": {
        "id": "nGmz0pRpBPaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Minibatch_NAdam(Weights, bias, L, X_train, y_train, eps, no_of_classes, batch_size, beta1, beta2, eta, max_epoch):\n",
        "\n",
        "  m_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  m_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "  v_w = weights(3, n, L, X_train, no_of_classes)\n",
        "  v_b = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "    \n",
        "  for epoch in range(max_epoch):\n",
        "      ## initialize the gradients of weights and biases\n",
        "      dW = weights(3, n, L, X_train, no_of_classes)\n",
        "      dB = biases(3, n, L, y_train, no_of_classes)\n",
        "\n",
        "      ## initialize the loss\n",
        "      cross_entropy_loss = 0\n",
        "      num_points_seen = 0\n",
        "\n",
        "      for x, y in zip(X_train, y_train):\n",
        "\n",
        "        #x = np.float128(x)\n",
        "\n",
        "        ## Forward propagation\n",
        "        a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L)\n",
        "        \n",
        "        cross_entropy_loss += -np.log(y_dash[y])\n",
        "\n",
        "        ## Backward Propagation\n",
        "        grad_W, grad_b = backward_propagation(a_out, h_out, y, y_dash, Weights, L)\n",
        "        \n",
        "        dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
        "        dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
        "\n",
        "        num_points_seen +=1\n",
        "\n",
        "        if num_points_seen%batch_size==0:\n",
        "          #compute intermediate values\n",
        "\n",
        "          m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
        "          m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
        "\n",
        "          v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
        "          v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
        "\n",
        "          m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
        "          m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
        "\n",
        "          v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
        "          v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
        "\n",
        "          # Weights updates\n",
        "          Weights = [Weights[i] - (eta/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "          # Biases updates\n",
        "          bias = [bias[i] - (eta/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
        "\n",
        "          dW = weights(3, n, L, X_train, no_of_classes)\n",
        "          dB = biases(3, n, L, y_train, no_of_classes)\n",
        "      \n",
        "      print(epoch, cross_entropy_loss)\n",
        "\n",
        "  return Weights, bias\n"
      ],
      "metadata": {
        "id": "GLRgSWWOGSeZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights, bias = Minibatch_NAdam(Weights, bias, L, X_train, y_train, 1e-10, no_of_classes, 25, 0.9, 0.999, 0.1, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYpytPVUn4PZ",
        "outputId": "9d681941-1f41-4899-8ddb-ae67255a4677"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 84607.72218064582286\n",
            "1 65691.58314440987447\n",
            "2 63625.163485619256303\n",
            "3 61345.835480034633015\n",
            "4 56382.25314378042691\n",
            "5 54782.47692052668505\n",
            "6 49958.795616479803282\n",
            "7 43365.806069021878436\n",
            "8 42394.361408850225175\n",
            "9 41293.58527524336861\n"
          ]
        }
      ]
    }
  ]
}